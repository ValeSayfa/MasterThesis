-- phpMyAdmin SQL Dump
-- version 5.0.2
-- https://www.phpmyadmin.net/
--
-- Host: 127.0.0.1
-- Generation Time: Nov 15, 2020 at 08:09 PM
-- Server version: 10.4.13-MariaDB
-- PHP Version: 7.4.8

SET SQL_MODE = "NO_AUTO_VALUE_ON_ZERO";
START TRANSACTION;
SET time_zone = "+00:00";


/*!40101 SET @OLD_CHARACTER_SET_CLIENT=@@CHARACTER_SET_CLIENT */;
/*!40101 SET @OLD_CHARACTER_SET_RESULTS=@@CHARACTER_SET_RESULTS */;
/*!40101 SET @OLD_COLLATION_CONNECTION=@@COLLATION_CONNECTION */;
/*!40101 SET NAMES utf8mb4 */;

--
-- Database: `migration`
--

-- --------------------------------------------------------

--
-- Table structure for table `contribution`
--

CREATE TABLE `contribution` (
  `id` int(11) NOT NULL,
  `Title` varchar(150) NOT NULL DEFAULT 'Title',
  `Year` year(4) NOT NULL DEFAULT 2000,
  `Authors` tinytext NOT NULL,
  `Link` varchar(250) NOT NULL DEFAULT 'Empty Link',
  `Approach` text NOT NULL,
  `Process_Rewrite` tinyint(4) NOT NULL DEFAULT 0 COMMENT 'Process Strategy',
  `Process_Extension` tinyint(4) NOT NULL DEFAULT 0 COMMENT 'Process Strategy',
  `Process_StranglerPattern` tinyint(4) NOT NULL DEFAULT 0 COMMENT 'Process Strategy',
  `Process_ContinuousEvolution` tinyint(4) NOT NULL DEFAULT 0 COMMENT 'Process Strategy',
  `Process_Split` tinyint(4) NOT NULL DEFAULT 0 COMMENT 'Process Strategy',
  `Process_Others` tinyint(4) NOT NULL DEFAULT 0 COMMENT 'Process Strategy',
  `ProcessDetail` mediumtext DEFAULT NULL,
  `Decomposition_DDD` tinyint(4) NOT NULL DEFAULT 0 COMMENT 'Decomposition Strategy',
  `Decomposition_FunctionalDecomposition` tinyint(4) NOT NULL DEFAULT 0 COMMENT 'Decomposition Strategy',
  `Decomposition_ExistingStructure` tinyint(4) NOT NULL DEFAULT 0 COMMENT 'Decomposition Strategy',
  `Decomposition_Others` tinyint(4) NOT NULL DEFAULT 0 COMMENT 'Decomposition Strategy',
  `DecompositionDetail` tinytext DEFAULT NULL,
  `TechniqueSet` mediumtext NOT NULL,
  `Technique_SCA` tinyint(4) NOT NULL DEFAULT 0 COMMENT 'Technique Type',
  `Technique_MDA` tinyint(4) NOT NULL DEFAULT 0 COMMENT 'Technique Type',
  `Technique_WDA` tinyint(4) NOT NULL DEFAULT 0 COMMENT 'Technique Type',
  `Technique_DMC` tinyint(4) NOT NULL DEFAULT 0 COMMENT 'Technique Type',
  `Technique_Others` tinyint(4) NOT NULL DEFAULT 0 COMMENT 'Technique Type',
  `TechniqueTypeDetail` mediumtext DEFAULT NULL,
  `Applicability_GR` tinyint(1) NOT NULL DEFAULT 0 COMMENT 'Applicability',
  `Applicability_MO` tinyint(1) NOT NULL DEFAULT 0 COMMENT 'Applicability',
  `AtomicUnit` tinytext DEFAULT NULL,
  `Input_SourceCode` tinyint(4) NOT NULL DEFAULT 0 COMMENT 'Input Choice',
  `Input_UseCase` tinyint(4) NOT NULL DEFAULT 0 COMMENT 'Input Choice',
  `Input_SystemSpecification` tinyint(4) NOT NULL DEFAULT 0 COMMENT 'Input Choice',
  `Input_API` tinyint(4) NOT NULL DEFAULT 0 COMMENT 'Input Choice',
  `Input_Others` tinyint(4) NOT NULL DEFAULT 0 COMMENT 'Input Choice',
  `InputDetail` tinytext DEFAULT NULL,
  `Output_List` tinyint(4) NOT NULL DEFAULT 0 COMMENT 'Output Choice',
  `Output_Archi` tinyint(4) NOT NULL DEFAULT 0 COMMENT 'Output Choice',
  `Output_Others` tinyint(4) NOT NULL DEFAULT 0 COMMENT 'Output Choice',
  `OutputDetail` tinytext DEFAULT NULL,
  `Tool` mediumtext DEFAULT NULL,
  `ProgramLanguage` mediumtext DEFAULT NULL,
  `Validation_Experiment` tinyint(4) NOT NULL DEFAULT 0 COMMENT 'Validation Type',
  `Validation_Example` tinyint(4) NOT NULL DEFAULT 0 COMMENT 'Validation Type',
  `Validation_CaseStudy` tinyint(4) NOT NULL DEFAULT 0 COMMENT 'Validation Type',
  `Validation_NoValidation` tinyint(4) NOT NULL DEFAULT 0 COMMENT 'Validation Type',
  `ValidationTypeDetail` mediumtext DEFAULT NULL,
  `ValidationMetrics` mediumtext DEFAULT NULL,
  `Quality_Maintainability` tinyint(4) NOT NULL DEFAULT 0 COMMENT 'Quality',
  `Quality_Security` tinyint(4) NOT NULL DEFAULT 0 COMMENT 'Quality',
  `Quality_Performance` tinyint(4) NOT NULL DEFAULT 0 COMMENT 'Quality',
  `Quality_Reliability` tinyint(4) NOT NULL DEFAULT 0 COMMENT 'Quality',
  `Quality_Scalability` tinyint(4) DEFAULT 0 COMMENT 'Quality',
  `Quality_Others` tinyint(4) NOT NULL DEFAULT 0 COMMENT 'Quality',
  `QualityDetail` mediumtext DEFAULT NULL,
  `Score` float NOT NULL DEFAULT 1
) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4;

--
-- Dumping data for table `contribution`
--

INSERT INTO `contribution` (`id`, `Title`, `Year`, `Authors`, `Link`, `Approach`, `Process_Rewrite`, `Process_Extension`, `Process_StranglerPattern`, `Process_ContinuousEvolution`, `Process_Split`, `Process_Others`, `ProcessDetail`, `Decomposition_DDD`, `Decomposition_FunctionalDecomposition`, `Decomposition_ExistingStructure`, `Decomposition_Others`, `DecompositionDetail`, `TechniqueSet`, `Technique_SCA`, `Technique_MDA`, `Technique_WDA`, `Technique_DMC`, `Technique_Others`, `TechniqueTypeDetail`, `Applicability_GR`, `Applicability_MO`, `AtomicUnit`, `Input_SourceCode`, `Input_UseCase`, `Input_SystemSpecification`, `Input_API`, `Input_Others`, `InputDetail`, `Output_List`, `Output_Archi`, `Output_Others`, `OutputDetail`, `Tool`, `ProgramLanguage`, `Validation_Experiment`, `Validation_Example`, `Validation_CaseStudy`, `Validation_NoValidation`, `ValidationTypeDetail`, `ValidationMetrics`, `Quality_Maintainability`, `Quality_Security`, `Quality_Performance`, `Quality_Reliability`, `Quality_Scalability`, `Quality_Others`, `QualityDetail`, `Score`) VALUES
(1, 'From monolith to microservices Lessons learned on an industrial migration to a Web Oriented Architecture', 2017, 'Jean-Philippe GOUIGOUX; Dalila TAMZALIT\n', 'https://ieeexplore.ieee.org/document/7958457', 'Clustering by the balance between the costs of Quality Assurance and the cost of deployment', 1, 1, 0, 0, 0, 0, 'Rewrite (deployment is automated) /Extension (deployment is not automated)', 0, 1, 0, 0, 'Business capabilities decomposition', '1) Choice of granularity (based on automated/ manual deployment scenarios)\r\n\r\n2) Deploy the services: Use Container-based technologies (Docker)\r\n\r\n3) Integrate the services: use of Webhooks, a light, unobtrusive, HTTPbased way of communicating between APIs for passive choreography', 0, 1, 0, 0, 0, 'The cost of QA & The cost of deployment (measured by time spent for validation and deployment)', 1, 1, 'Cost-based determination of granularity of services', 1, 0, 1, 0, 0, 'Non-functional requirement (Quality assurance and deployment costs) & Source code', 0, 1, 0, NULL, '1) Docker Compose (deployment) + webhooks(integration)\r\n\r\n2) API based communication, configuration by YAML', NULL, 0, 0, 1, 0, 'Real production projects\n', NULL, 1, 0, 1, 0, 0, 1, 'Maintainability(Reusability, Modifiability), Performance efficiency (Time behavior), Other (level of support), Increase of reuse, Replacement facilitated, Performance increases(response time), Lower levels of support, Financial return over increase', 2),
(2, 'A Probabilistic Approach For Obtaining An Optimized Number Of Services Using Weighted Matrix And Multidimensional Scaling', 2017, 'Anika Sayara; Md. Shamim Towhid; Md. Shahriar Hossain', 'https://ieeexplore.ieee.org/document/8281804', 'Towards business capability decomposition to migrate from monolithic to microservices, using multidimensional scaling and weighted matrix', 0, 0, 0, 0, 0, 1, NULL, 0, 1, 0, 0, 'Business capabilities decomposition', '1) Get original services\r\n\r\n2) Decompose  Sub-business capability\r\n\r\n3) Measure update probabilities/ scale up probabilities/ scale down probabilities\r\n\r\n4) Calculate UpdateMatrix\r\n\r\n5) Calculate ScalingMatrix\r\n\r\n6) Group together the sub-business capabilities having similar weight by applying the same techniques\r\n', 0, 1, 0, 0, 0, 'Based on update probabilities/ scale up probabilities/ scale down probabilities as well as implementation technology to decide what services should be clustered', 0, 1, 'Sub-business capability, granularity based on Algorithm1', 0, 1, 0, 0, 1, 'Version control history data & business case diagram or the planning of application', 1, 0, 0, 'Microservice structure diagram', NULL, 'Generic', 0, 1, 0, 0, 'Decomposition based on a fictional online reading application\r\n\r\n1) Fictional online reading application\r\n\r\n2) result in 5 microservices\r\n\r\n3) perform each steps proposed by the contribution and present the example result\r\n', NULL, 1, 0, 0, 0, 1, 0, NULL, 3),
(3, 'Microservices architecture: Case on the migration of reservation-based parking system', 2017, 'Pujianto Yugopuspito; Frans Panduwinata; Sutrisno Sutrisno', 'https://ieeexplore.ieee.org/document/8359946', 'Towards the classification of services and decompose the monolithic application', 0, 0, 0, 0, 1, 0, NULL, 1, 0, 0, 0, NULL, 'SERVICE ORIENTED ARCHITECTURE (SoaML)\r\n\r\n1) Classify the services\r\n\r\n2) Identify the service participants (either provider or requester)\r\n\r\n3) Produce a service contract diagram: define the agreement between participants, about how a service to be provided\r\n\r\n4) Draw the service architecture diagram\r\n\r\n5) splitting of the services in the monolith\r\n\r\n\r\n\r\nMICROSERVICE ARCHITECTURE\r\n\r\n1) Separate in database to ensure independent of each other: two principles for microservices single responsibility and interface segregation principles\r\n\r\n2) Provide REST API as the interface for synchronous messaging communication, implement the common nonfunctional requirement (security, monitoring, and throttling), implement OAuth2 and OpenID Connect into the migrated microservices architecture as an API security standards\r\n\r\n3) Implement service registry for to find the available microservices: server side discovery and client-side discovery\r\n\r\n4) Adopt self-managing atomic service to ensure the health management of the system: Load balancing\r\n', 1, 0, 0, 0, 0, 'Decomposition based on service contract diagram and service architecture diagram', 0, 1, 'Services, granularity based on two principles: single responsibility, interface segregation principles', 0, 0, 0, 0, 1, 'Service contract diagram and service architecture diagram\n', 0, 0, 1, 'Service diagram of microservice application', NULL, 'Generic', 0, 0, 1, 0, '1) Reservation-based Parking System (Mobile app,2016)\r\n\r\n2) result in 4 microservices\r\n\r\n3) decompose the monolithic application', NULL, 1, 0, 0, 0, 1, 1, 'Scalability, Modularity, Modifiability', 2.5),
(4, 'Functionality-oriented Microservice Extraction Based on Execution Trace Clustering\n', 2018, 'Wuxia Jin; Ting Liu; Qinghua Zheng; Di Cui; Yuanfang Cai\n', 'https://ieeexplore.ieee.org/document/8456351\n', 'Functionality-oriented Microservice Extraction Based on Execution Trace Clustering', 0, 0, 0, 0, 1, 0, 'Grouping similar functionalities (business logic) as a service', 0, 1, 0, 0, NULL, '1) Execution Monitoring: The Instrumented System is executed using the pre-prepared Test Cases to get Log Files with Kieker\r\n\r\n2) Execution Trace Analysis: From Log Files, we analyze execution traces to generate Class-level and Method-level Execution Traces\r\n\r\n3) Microservice Extraction: Functionality oriented microservice extraction (FoME) method,  Algorithms are proposed to generate Microservice Candidates that consist of classes, interfaces and application programming interfaces (APIs).\r\na) Class clustering (Trace Clustering Algorithm1): Group the classes dedicated to the same business logic as the skeleton of one microservice by clustering the execution traces\r\nClass-level Execution Traces -> Clusters of Class-level Execution Traces.\r\nb) Crossover processing : Several classes are clustered into more than one service, called shared classes or crossovers. Some shared classes can also be extracted to a separate service.\r\nc) Microservice candidate generation: identify Interfaces and corresponding fine-grained APIs for each extracted microservice candidate\r\nClusters of Class-level Execution Traces + Method-level Execution Traces-> Interfaces and corresponding fine-grained APIs for each extracted microservice candidate.\r\n', 0, 0, 1, 0, 0, 'Microservice Extraction Based on execution log files', 0, 1, 'Microservice candidates, granularity based on Algorithm\n', 0, 0, 0, 0, 1, 'Instrumented System & Test cases\n', 0, 1, 0, 'Loosely coupled microservices', '1) Kieker 1.13 for Execution Monitoring, Algorithm (Self-developed) for Execution Trace Clustering, FoME (Self-developed Algorithm) for Microservice Extraction\n\n2) Java EE systems, tool is half-automated', 'Generic', 0, 0, 1, 0, '1) JPetstore: e-store application, 1438 LOC, 24 classes, 15 test cases; \r\nSpringblog: blogging application, 3583 LOC, 85 classes, 33 test cases; \r\nJForum: forum application, 29550 LOC, 340 classes, 69 test cases; \r\nRoller: blogging application, 47602 LOC, 534 classes, 87 test cases\r\n\r\n2) Jpetstore: 3 microservices are Service Account, Service Product, and Service Order\r\n\r\n3) a) Compare FoME with state-of-the-art methods: LIMBO, WCA and MEM\r\nb) Coverage analysis for classes that are covered in the execution traces\r\nc) Evaluation results (cohesion and coupling) by means of metrics: CHD, CHM, IFN, OPN, IRN\r\n', 'Maintainability-Modularity (functional independence)', 1, 0, 0, 0, 0, 1, 'Loosely coupled microservices and aid the developer during migration and reduce effort', 4),
(5, 'Transform Monolith into Microservices using Docker\n', 2017, 'Sarita; Sunil Sebastian\n', 'https://ieeexplore.ieee.org/document/8463820\n', 'Microservices transformation and deployment strategy using Docker which is based on container based virtualization', 0, 0, 0, 1, 0, 0, NULL, 0, 0, 1, 1, 'Existing System’s Structure (frontend and backend modules as independent services) & module decomposition', '1) Partition Frontend and Backend\r\n\r\n2) Decouple the modules as services: look up their coarse grained boundaries and dependencies\r\na) Decompose based on role in the business value.\r\nb) Decompose based on change frequency.\r\nc) Decompose based on resource requirements. (Modules using in-memory database will become one service on Docker or VMs; Modules with computational expensive algorithms on  instances having higher number of CPU)\r\n\r\n3) Build Docker images for those independent microservices\r\n\r\n4) Docker Containers are created with these Docker images\r\n\r\n5) Each microservice image is deployed on a container and it can be called as a microservice instance and the constituent containers are linked together to form the cohesive application.\r\n', 0, 1, 0, 0, 0, 'Decompose based on role in the business value, change frequency and resource requirements', 0, 1, 'Self-contained microservices in Docker\r\n', 1, 0, 0, 0, 0, NULL, 0, 0, 1, 'Set of Images in Docker\n', 'DOCKER: An open platform for developing, deploying and shipping applications\r\n', 'Generic', 0, 0, 0, 1, NULL, NULL, 1, 0, 0, 0, 1, 1, 'Independence, Automation, Scalability, Reduce effort, Reduce complexity, Fault tolerance\r\n ', 1),
(7, 'Visualization Tool for Designing Microservices with the Monolith-first Approach\n', 2018, 'Rina Nakazawa ; Takanori Ueda ; Miki Enoki ; Hiroshi Horii\n', 'https://ieeexplore.ieee.org/document/8530129\n', 'Visualization Tool for Designing Microservices from monolithic applications: for Java, Go, Node.js', 0, 0, 0, 0, 1, 0, NULL, 0, 1, 0, 1, 'Functional Decomposition & Class Decomposition\n', '1) Component Dependency Extraction: \r\nProfile data is used to construct a calling context tree (CCT), indicating an estimated amount of communication between microservices. The algorithm in List 1 generates a compacted CCT from profile data by matching function names with keywords.\r\n\r\n2) Microservice Clustering: \r\nAn initial design of microservices is generated by employing the two proposed clustering techniques. The clustering techniques use text features in source codes or call chains in a CCT.\r\na) Semantic Clustering: considers text features in source files, composes a microservice with similar classes in terms of the content of each class (class-level clustering)\r\nb) CCT-based clustering: considers amount of communication, put functions of the same class into different microservices (function-level clustering)\r\n\r\n3)Visualization and Refinement:\r\nVisualized manual optimization  by recommendation. Visualization tool relays on package names to find components in the application layer.\r\na) Visualization of Microservice Design: transforms a compacted CCT into a class-dependency graph using the proposed tool.\r\nb) User Interaction for Refining Microservice Design: Create, Move, Clone, Keep (four-choice actions for class operation, the number of clusters is increased or decreased)\r\nc) Recommendation of Refinement Action: The visual interface displays a list that shows classes in descending order of total number of function calls to/from classes in other microservices, recalculates the recommendation dynamically after a user takes one of the refinement actions, presents the action that reduces the number of API calls in the greatest possible manner.', 1, 0, 1, 1, 0, 'A visualization tool automatically generated from a monolithic prototype application. To estimate the amount of communication between components, it uses calling contexts collected by a profiler in a dry run of the prototype. Developers can refine the generated design through interactions with the visual interface, and that refinement helps avoid unintentional performance degradation', 0, 1, 'Function sets/ Class sets, granularity based on user optimization\n', 1, 0, 0, 0, 1, 'Profile data &  Source code & Commit information\n', 0, 1, 0, NULL, 'A visualization tool proposed by author: for Java, Go, and Node.js', 'Java, Go, and Node.js', 0, 0, 1, 0, '1) Acme Air: benchmark application that mimics a flight reservation, middleware like Docker and MongoDB, 2234 LOC, since 2013; \r\nDayTrader: Java EE benchmark that simulates an online stock-trading website, MVC mode, 4977 LOC, for 13 years.\r\n\r\n2) Semantic based clustering (refinement): 4 microservices; \r\nCCT-based clustering (refinement): 2~3 microservices\r\n\r\n3) After implementation of the tool, compare the initial clustering by two algorithms. Calculate the similarity, amount of function calls, number of click counts (changes) before and after microservice refinement.\r\n', 'Performance (Time behavior [amounts of communication]), Flexibility (Modularity[Number of API calls], Modifiability [granularity, similarity score]), Usability [NUMBER OF MENU CLICKS]\r\n', 0, 0, 1, 0, 0, 0, 'Trade-off between service granularity and runtime performance, flexibility of development\n', 4.5),
(9, 'Microservices: Migration of a Mission Critical System\n', 2018, 'Manuel Mazzara; Nicola Dragoni; Antonio Bucchiarone; Alberto Giaretta; Stephan T. Larsen; Schahram Dustdar\n', 'https://ieeexplore.ieee.org/document/8585089\n', 'Using Docker and GoCD platform, RabbitMQ, Redis, etc for microservice implementation\n', 1, 0, 0, 0, 0, 0, 'Completely replace the old MA. The migration was performed manually, considering each specific functionality and identifying whether it should have resulted in a new service or not\r\n ', 0, 1, 0, 0, 'Business-driven (business functionality clustering) and out-side-in\r\n', 'Functional decomposition (When a business functionality is isolated and sufficiently big, or shared among many other business functionalities, it should result in a new service)', 0, 0, 0, 0, 1, '', 0, 1, 'Business functionality, granularity decided manually\r ', 0, 0, 0, 0, 1, '', 0, 0, 1, 'Microservice application', '1) Containerization: Linux Containers on the Docker Swarm cluster. All container images are hosted on an internal Docker Registry. A list of all FX Core images can be retrieved with a search within the local registry.\r\n\r\n2) Automation: All services has an automated continuous integration and continuous deployment (CICD) pipeline on their internally hosted GoCD server. The GoCD platform offers a simple interface, which gives an overview and interaction with building, testing and deployment.\r\n\r\n3) Orchestration: Docker Swarms orchestration on the swarm cluster.\r\n\r\n4) Clustering: The architecture runs across five virtual hosts located in the three data-centers. On each host a Docker Swarm cluster has been set up, with each host acting as a Swarm Node. This allows the three Docker Engines, allowing containers to run spread across the cluster. Docker Swarm defines internal networks which is used to communicate between service containers\r\n\r\n5) Load Balancing and Service Discovery: Load balancing is therefore required to be implemented by both RabbitMQ and Swarm service discovery.', NULL, 0, 0, 1, 0, 'Danske Bank (Foreign Exchange): result in 10 micro-services, Danske Bank FX Core MSA is hosted on private data centers. There are three data-center locations in Denmark, to achieve better availability and increased resilience to the internal systems. The infrastructure consists of VMs ordered through a web-portal, manually setup by the FX Core team.\r\n', 'Reliability (Availability), Capacity (System Load), Maintainability (Complexity), etc (Administrative costs, Consistency, Heterogeneity)\r\n', 1, 0, 0, 1, 1, 1, 'Scalability, maintainability, reduce complexity, better availability and increased resilience to the internal systems.\nIncreasing code reuse and robustness, shift towards distribution, modularization, and loose coupling.increasing software quality, reduce cost\n', 2),
(11, 'Extracting Candidates of Microservices from Monolithic Application Code\n', 2018, 'Manabu Kamimura ; Keisuke Yano ; Tomomi Hatano ; Akihiko Matsuo\n', 'https://ieeexplore.ieee.org/document/8719439\n', 'Extracting Candidates of Microservices from code analysis in combination with data access: for JAVA and COBOL\n', 0, 0, 0, 0, 1, 0, 'Splitting the existing code and database\r\n', 0, 0, 1, 0, NULL, '1) Extract the dependencies of program groups and data: Dependencies between programs and dependencies between programs and data\r\nUse program name or class name to define entry points, use static code analysis to extract dependencies from source code, use the information of program calls or data access which can be collected by dynamic analysis. Group all the dependent programs related to each entry point and define them as Program Groups.\r\n\r\n2) Clustering using the relationship between program groups and data: \r\nAnalyze the relationship of data and Program Groups and classify them as read access and write access. \r\na) Employ software clustering to program and data by proposed algorithm or by SArF software clustering algorithm [9] with data access [10], output a dendrogram\r\nb) Generate Abstract Tree Model of features from dendrogram\r\nc) Lay out the classes in to a City Block Diagram\r\nd) Lay out corresponding features to the City Block Diagram\r\ne) Assign overlay information (eg. package information) \r\nf) Group the related data together by dedication score \r\n\r\n3) Splitting off the common data\r\n\r\n4) Visualize the microservice structure (SArF Map)\r\n', 1, 0, 1, 0, 0, 'Analysis the source code with information as \"entry points\", which is the user\'s interaction points defined as the APIs, batch execution points or front-end screens\n\nUse the information of program calls or data access which can be collected by dynamic analysis', 0, 1, 'Program groups or Data (tables /files), granularity decided by algorithm\n', 1, 0, 0, 0, 1, 'Source code & data\r ', 1, 0, 1, 'Candidates of microservices as the list of programs and data\n', 'SArF and SArF Map for clustering and visualization: for JAVA and COBOL\n', 'Java and COBOL', 0, 0, 1, 0, '1) Spring Boot Pet Clinic: Downloaded on Feb. 16, 2018, Java, 630 LOC, 25 Java Class; Files\r\nIndustrial application for purchase operation: Migrated about 12 years ago, COBOL, 2,031 KLOC, 2,269 COBOL Files\r\n\r\n2) Spring Boot Pet Clinic: 3 microservices; \r\nIndustrial application: 199 microservices\r\n\r\n3) Validation Steps: Compare with existing Microservice Architecture or review by developers, compare data access (Loose coupling) between the results by proposed microservice extraction method or  other existing method (Grouping by name)\r\n', 'Maintainability (Number of data access-loose coupling); Mutual dependencies between candidates: Number of microservice candidate, Read access, Write access, Sum of access; Number of elements extracted (entry points and data); Number and size of the extracted candidates', 1, 0, 0, 0, 0, 1, 'Adaptable to change, maintenance (loose coupling)\r\n', 4),
(13, 'Attributes Assessing the Quality of Microservices Automatically Decomposed from Monolithic Applications\n', 2019, 'Michel-Daniel Cojocaru; Alexandru Uta; Ana-Maria Oprescu\n', 'https://ieeexplore.ieee.org/document/8790889\n', 'Attributes Assessing the Quality of Microservices', 0, 0, 0, 0, 0, 1, '', 1, 0, 1, 1, 'Domain-Driven Design/ Architecture-Centric Modeling/ Data Flow Diagram\n', 'Service Cutter, Microservice Identification Through Interface Analysis\n', 1, 0, 0, 1, 0, 'Static Analysis: Scanning the code of a microservice before being linked to other microservices \n\nDynamic Analysis: employed when the application code is not available. The most common type of dynamic analysis consists of Unit Tests', 0, 0, NULL, 0, 0, 0, 0, 1, NULL, 0, 0, 0, NULL, NULL, NULL, 0, 1, 1, 0, 'Meta study and interview with expert\r\n\r\nScientific research about included quality attributes and interview with expert, idea for minimum set of quality attributes applicable to microservice resulted from semi-automatic decomposition\r\n', 'Static Analysis: Granularity, LOC, Open Interface, High Cohesion, Loose Coupling\r\n\r\nDynamic Analysis: Execution Cost, Response Time, Availability, Successful Execution Rate, Usage Frequency, Scalability, Independence, Maintainability, Deployment, Health Management, Modularity, Manageability, Performance, Reusability, Technology Heterogeneity, Agility, Security, Load Balancing, Organizational Alignment\r\n\r\nMeta Study: Granularity, Cohesion, Coupling, Execution Cost, Response time, Availability, Successful execution rate, Usage Frequency, Scalability, Independence, Maintainability, Deployment, Health Management, Modularity, Manageability, Performance, Reusability, Technology Heterogeneity, Agility, Security, Load Balancing, Organizational Alignment\"\r\n', 1, 1, 1, 1, 1, 1, 'Cost related implications as well as challenging and complex tasks, agility or scalability, ensure quality and critical attributes of the software', 3),
(14, 'Automatic performance monitoring and regression testing during the transition from monolith to microservices\n', 2019, 'Andrea Janes; Barbara Russo\n', 'https://ieeexplore.ieee.org/document/8990249\n', 'Automatic performance monitoring and regression testing during the transition from monolith to microservices: for Microsoft .NET/ Aspect Oriented Programming/ JavaScript\r\nWindows, Mac OS, and Linux\r\n', 0, 0, 0, 1, 0, 0, NULL, 0, 0, 0, 1, '', '', 0, 0, 1, 0, 0, 'Performance data: response times for different usage intensities\nStack traces: user interactions, number of accesses to each service, current user, Timestamp', 1, 1, NULL, 0, 0, 0, 0, 1, '', 0, 0, 1, '', '1) Faban load driver framework: run the tests and collect the performance measures.\r\n\r\n2) PPTAM+ (Production and Performance Testing Based Application Monitoring for workload) + Application Performance Monitoring (APM) tool for performance data, stack traces and visualization\r\n\r\n3) openAPM: online tools\' configurator that helps select open source applications to build an APM tool\r\n\r\n4) RShiny of RStudio: a web framework for building web applications using R\r\n\r\n5) Elastic: Performance Monitoring\r\n\r\n6) openTracing: collect stack traces\r\n\r\n7) Kibana/ Grafana: a web user interface that visualizes complex aggregations, graphs, and charts\r\n\r\n8) Apache Lucene: search engine can search and aggregate performance data\r\n\r\n9) InfluxDB: store analysis of time series data\r\n\r\n10) Jupiter13 notebooks: scripts in R11 (or Python12), performance data analyze and result presentation + exchange data between the components of the PPTAM+ tool\r\n\r\n11) Load testing process: proposed by Schulz et al. [17]', 'Microsoft .NET/ Aspect Oriented Programming/ JavaScript\r\nWindows, Mac OS, and Linux', 0, 1, 0, 0, NULL, 'Performance\n', 1, 0, 1, 0, 1, 0, NULL, 4),
(26, 'Migrating Web Applications from Monolithic Structure to Microservices Architecture\n', 2018, 'Zhongshan Ren; Wei Wang; Guoquan Wu; Chushu Gao; Wei Chen; Jun Wei; Tao Huang\n', 'https://dl.acm.org/doi/abs/10.1145/3275219.3275230\n', 'Migrating Web Applications from Monolithic Structure to Microservices Architecture', 0, 0, 0, 1, 0, 0, NULL, 0, 1, 0, 0, NULL, '1) Static analysis: (static interaction model) get the abstract syntax tree (AST), class hierarchy architecture (CHA) and invoking relationship between functions (call graph), build the dependency graph of the application modules, classes and functions\r\n\r\n2) Dynamic analysis: (runtime dynamic interaction model) get system log, user behavior, dynamic scheduling frequency, error information\r\n\r\n3) Constructing the candidate set of microservices: determine microservices boundaries based on service capability and characteristics of a microservice, use transition probability to evaluate the connection and jump probability between classes and methodes, extract microservices from the monolith application based on clustering according to the value of the coupling evaluation function (distance and correlation function provided in the contribution), use Jaccard similarity coefficient to measure the similarity of different decomposition scenario, use SSE (the Sum of the Squared Error) to evaluate the accuracy of clustering results\r\n\r\n4) Migration: basement functional components will be replaced by new components build with new technology stack\r\n\r\n5) Data migration: use a self-developed monitoring and tracing tool called EasyAPM  to record the data operation and the parameter information through the instrument on the JDBC and data access class libraries, divide the data to multi-part according to the data access nearby principle, in order to ensure data consistency\r\n\r\n\r\n', 1, 0, 1, 0, 0, 'Use static analysis of the application source code to get the static trait of the monolithic system, and obtained the runtime dynamic features through the dynamic tracing', 0, 1, 'Minimal unit that can be the result of a decomposition\r\n', 1, 0, 1, 0, 1, 'Source codes & design documents & developer information & runtime log\r\n ', 0, 0, 1, 'Microservice partition diagram\n', 'Use a self-developed monitoring and tracing tool called EasyAPM to record the data operation and the parameter information through the instrument on the JDBC and data access class libraries\n', 'Generic', 1, 0, 1, 0, '1) Check P.8 Table 1\n\n2) Check P.8 Table 1\n\n3) compare performance between monolith and microservices applications by selecting different loads in nearly the same scenarios (Check P.8 Table 1), compare performance of legacy application feature analysis by PA (package analysis), SSA (static structure analysis), CHA (class hierarchy analysis), SCGA (static call graph analysis), and CSDA (combined static and dynamic analysis), (Check P.9 Table 3)\n', 'Performance (throughput, load, time)\r\n', 1, 0, 1, 0, 1, 1, 'Scalability, performance, upgrade and maintenance\n', 4),
(31, 'A logical architecture design method for microservices architectures\n', 2019, 'Nuno  Santos; Carlos E Salgado; Francisco Morais; Mónica Melo; Sara  Silva; Raquel  Martins; Marco  Pereira; Helena  Rodrigues; Ricardo J. Machado; Nuno  Ferreira; Manuel  Pereira\n', 'https://dl.acm.org/doi/abs/10.1145/3344948.3344991\n', 'Four-Step Rule Set (4SRS) for designing microservices-oriented logical architecture (MSLA)', 1, 0, 0, 1, 1, 0, NULL, 1, 0, 0, 0, NULL, '1) Requirements modeling: decomposing functionalities based on specific tasks in a tree-like form, results in a structure for specifying lower-level details on each requirement, have a logical view of the microservices\' internal behavior and communications (Output Use Case Diagram)\r\n\r\n2) Components Creation (4SRS Step1): For each use case, create components such as interface+data model + logic/control (Output MVC Model)\r\n\r\n3) Components Elimination (4SRS Step2):  Eliminate redundant components and add name and description for each maintained component\r\n\r\n4) Component Packaging (4SRS Step3): Grouping a set of components in packages, which further compose higher-level microservices\r\n\r\n5) Microservices Associations (4SRS Step4): depict associations relate to service channels that exist in order to allow communication between microservices, define how the service behaviors internally, but also how they communicate between them\r\n\r\n6) Representation of MSLA in SoaML: use service-oriented architecture modeling language (SoaML) to describe service-based architectures, like Service Participants, Capabilities, Service Architectures, and Service Interfaces.\r\n\r\n7) Microservice development\r\n', 1, 0, 0, 0, 0, 'This approach uses UML use cases diagrams for domain modeling', 1, 1, 'Set of business functions, database table and interface\n', 0, 1, 0, 0, 0, NULL, 0, 1, 0, 'Overview of microservices-oriented logical architecture (MSLA)\n', NULL, 'Generic', 0, 0, 1, 0, '1) The Integrated Management Platform 4.0: an ERP system for the textile production domain\n\n2) Result in 12 microservices\n\n3) Validation Steps Not specified\n', NULL, 0, 0, 0, 1, 1, 0, 'Scalability, availability or reliability', 3.5),
(32, 'Availability and Scalability Optimized Microservice Discovery from Enterprise Systems\n', 2019, 'Adambarage Anuruddha Chathuranga De Alwis; Alistair Barros; Colin Fidge; Artem Polyvyanyy\n', 'https://link.springer.com/chapter/10.1007/978-3-030-33246-4_31\n', 'Multiple Objective Optimized Microservice Discovery from Enterprise Systems (scalability, availability, efficiency, high cohesion and low coupling)', 0, 0, 0, 0, 1, 0, NULL, 0, 0, 0, 1, 'Business Objects Grouping', '1) Perform static analysis to derive the business objects\r\na) extract and evaluate all the SQL queries\r\nb)identify the relationships between database tables to derive the business objects (Nooijen et al. [10])\r\n\r\n2) Perform behavioral analysis (simulating the users\' behavior with the help of Selenium scripts) to generate system execution logs and call graphs\r\n\r\n3) Perform optimization algorithm to discover a high performing system partitioning for microservices: Non-dominated Sorting Genetic Algorithm II (NSGA II) which is a multi-objective  optimization algorithm which provides an optimal set of solutions while achieving global optima (algorithm P10)\r\na) SYNPOP function which synthesizes a parent population of the given size n, calculates and stores the fitness for each parent (high cohesion, low coupling, availability, scalability)\r\nb) generates the child population for \"Gen\" times by performing crossover operations and mutation operations on the parent chromosomes, calculates non-dominated fronts (Pareto fronts), concatenating the ranked fronts\r\nc) return ranked fronts which constitutes the clustering of business objects  and operation nodes in the system to develop Microservices', 1, 0, 1, 1, 0, 'Static analysis techniques and dynamic analysis techniques to identify suitable consumer-oriented parts of enterprise systems through business object relationships and their execution patterns to reengineer as microservices analyze their scalability and availability characteristics to provide better microservice configurations.\r\n\r\nNSGA II Algorithm for iteratively optimization of microservice discovery', 0, 1, 'Node illustrates an operation executing in the system and the business objects illustrate the business objects that each operation executes on, granularity based on algorithm\n', 1, 0, 0, 0, 1, 'Source code & database\n', 0, 0, 1, 'Clustering of business objects and operation nodes', '1) Selenium scripts (simulating the users\' behavior)\r\n\r\n2) Disco process mining tool (Analysis execution logs from XES format files to generate call graphs)\r\n', 'Generic', 1, 0, 0, 0, '1) SugarCRM is a customer relationship management system contains 8116 source files and 600 attributes divided between 101 tables; \r\nChurchCRM is a customer relationship management system contains 8039 source files and 350 attributes divided between 55 tables\r\n\r\n2) SugarCRM result in 8 Microservices;  ChurchCRM result in 11 Microservices\r\n\r\n3) host the SugarCRM and ChurchCRM systems in AWS Cloud and host data in a MySQL relational database, measure, calculate and compare coupling, cohesion, availability and scalability of original system, optimized microservice and wrongly implemented microservices\r\n', NULL, 0, 0, 1, 1, 1, 1, 'Modularity (coupling, cohesion), Performance (efficiency), Reliability (availability), scalability', 4.5),
(34, 'A Model-Driven Approach Towards Automatic Migration to Microservices\n', 2020, 'Antonio Bucchiarone; Kemal Soysal; Claudio Guidi\n', 'https://link.springer.com/chapter/10.1007/978-3-030-39306-9_2\n', 'A Model-Driven Approach Towards Automatic Migration to Microservices: for Java-based application to Jolie - a programming language for defining microservices, implemented by means of JetBrains MPS', 0, 0, 0, 0, 1, 0, NULL, 0, 0, 0, 1, 'Model-Driven Approach, expressing domain requirements by appropriate domain specific languages\n', '1) Source Code Importing\r\nImport and parse the monolithic Java source code into MPS (text-based meta-programming system) base language and imported in the MPS editor as input Java models.\r\n\r\n2) Microservice Mining\r\nImplement  the Microservices Miner component to search in the abstract syntax tree (AST) of the imported Java models and identify set of microservices for the migration\r\n\r\n3) Microservice Modeling\r\nCreate models of the identified microservices with Microservice DSL (domain specific language)\r\n\r\n4) Jolie Files Gernation\r\nUse Microservices Generator, the transformation feature (AST to text (model-to-text) transformation) provided by MPS, microservices models with their respective interfaces are transformed into the target Jolie .ol and .iol files\r\n\r\n5) Docker File Generation\r\nA Dockerfile is created according to each Jolie microservice and used to deploy the overall application in a Docker container', 1, 0, 0, 0, 0, 'Migration framework is realized by means of Jet-Brains MPS (briefly, MPS) to first analysis the Java source code', 0, 1, 'Expression as nodes (patterns)  in the AST that comply to each predefined semantic representing specific features (functional units)\n', 1, 0, 0, 0, 0, 'Java source code', 0, 1, 0, 'Microservice Architecture (developed using Jolie) and the corresponding deployment in a Docker Container\n', '1) JetBrains MPS: a meta-programming framework that can be exploited as modelling languages workbench, it is text-based, and provides projectional editors, supports definition of microservice mining, specification, and deployment phases for the migration\r\n\r\nJolie programming language: offers a native linguistic tool for defining microservices following a structured service oriented paradigm\r\n\r\n2) Docker container: supports Microservice deployment\r\n\r\n3) Self-developed MPS editor: helps developers to specify Dockerfile models and deploy Docker containers\r\n', 'Java-based application to Jolie - a programming language for\r\ndefining microservices, implemented by means of JetBrains MPS', 0, 1, 0, 0, NULL, NULL, 0, 0, 0, 0, 0, 0, NULL, 4.5),
(39, 'Towards a Methodology to Form Microservices from Monolithic Ones\n', 2017, 'Gabor Kecskemeti; Attila KerteszAttila; Csaba Marosi\n', 'https://link.springer.com/chapter/10.1007/978-3-319-58943-5_23\n', 'Image repository management of multiple federated clouds in the frame of the ENTICE project for transforming monolithic services to microservices', 0, 0, 0, 1, 0, 1, 'Generic monolithic images modification to optimized image for microservices.\nTransform distributing Virtual Machine and Container Images (VMIs) within the ENTICE repository to an optimized one that holds only the intended microservice functionality and extract them\n', 0, 1, 0, 0, NULL, '1) Image synthesis\r\na) Image creation\r\nb) Build/bootstrap\r\nc) Build/provision\r\nd) Test\r\n\r\n2) Image analysis \r\na) Equivalent pieces discovery\r\nb) VM images fragmentation\r\nc) Information and VMI storage\r\n\r\n3) Targeted Size Optimization:\r\na) Functionality test for the required microservice preparation\r\nb) Original image is instantiated (virtualized environment (VE)) setup by new VM or container instantiation\r\nc) VE’s read access operations collection and microservice’s functionality test execution\r\nd) Partially optimized image (PO image) uploading\r\ne) Optimizer VM deployment to use PO image\r\nf) Optimization procedure by executing a test script\r\ng) Optimizer VM uploads the new image to the cloud\r\nh) Repeating optimization processes until final PO image is given back to the ENTICE environment\r\n', 1, 0, 0, 0, 0, 'Users provide their original and functionally complete VM or container images\r\n\r\nThen use ENTICE to transparently tailors and optimizes the images for user targeted Cloud infrastructures with respect to their size, configuration, and geographical distribution', 0, 1, 'VM and container images, granularity is based on imagine optimization\r\n', 0, 0, 0, 0, 1, 'Original and functionally complete monolithic VM or container images\n', 0, 0, 1, 'Micro-service image family\n', 'ENTICE project: A multidisciplinary team of computer scientists, application developers, cloud providers and operators with the aim to research a ubiquitous repository-based technology for VM and container image management called ENTICE environment. This environment proves a universal backbone for IaaS VM/container image management operations, which accommodate the needs for different use cases with dynamic resource (e.g., requiring resources for minutes or just for a few seconds) and other QoS  requirements, it provides constant support for applications via optimized VM image creation, assembly, migration and storage.\r\n', NULL, 0, 0, 0, 1, NULL, NULL, 1, 0, 0, 0, 0, 0, 'Improving resource utilization, maintainability, cost and effort\r\n ', 3),
(40, 'Translating a Legacy Stack to Microservices Using a Modernization Facade with Performance Optimization for Container Deployments\n', 2020, 'Prabal Mahanta; Suchin Chouta\n', 'https://link.springer.com/chapter/10.1007/978-3-030-40907-4_14\n', 'An approach to establish platform independent interfaces, bounded domain contexts, eliminating non-critical legacy components and incremental quality aware methods to translate a legacy system to microservices', 0, 0, 0, 1, 1, 0, 'Splitting existing code parts, incremental quality aware methods', 0, 0, 0, 1, 'Decompose system with artifacts representing either of the following:\r\na) Function Modules\r\nb) Inter Function Calls\r\nc) Data relationships', 'See P.7  Fig5.\r\n1) Summarize the  core business requirements (UML, Source Code)\r\n\r\n2) Analysis and impact of business processes on systems, architectures and dependencies. Structural and Flow analysis and mapping of abstracted business data on control flow of the graph edges (AST Abstract syntax tree)\r\n\r\n3) Composition Generation (Composition Determination)\r\n\r\n4) Simulations (traditional c packages code flow analysis for evaluating performance, determination of functional and data dependencies): Data dictionary, Metrics at class, and function level, Invocation tree, Cross-reference objects, Unused functions and objects\r\n\r\n5) SQL queries, database optimization: Adjacency list, Path enumeration, Nested sets, Transitive closure, Warshall’s Algorithm [7] and Warren’s algorithm[8], etc.\r\n\r\n6) Interface analysis and refactoring', 1, 0, 0, 0, 0, 'Based on UML and source code, to analysis core business requirement for further modernization process', 0, 1, 'System artifacts including: Function Modules, Inter Function Calls, Data relationships', 1, 0, 0, 0, 1, 'UML and source code\n', 0, 0, 1, 'Decomposed system artifacts with optimized databases and functional dependencies\r\n', NULL, 'Generic', 0, 1, 0, 0, '1) SmartCity (demo) application\n\n2) Resulting number of services not specified\n\n3) Sequence diagram of specific service post modernization Activity diagram, overall flow with data generation optimizations', 'Achieved 70% performance boost, saves a researcher’s time and effort.experience better\n', 1, 0, 1, 0, 1, 1, 'Improve our architecture based on key parameters like performance and scalability, maintainability, incorporate new features fast enough\n', 3),
(41, 'From Monolith to Cloud Architecture Using Semi-automated Microservices Modernization\n', 2019, 'Salvatore Augusto Maisto; Beniamino Di Martino; Stefania Nacchia', 'https://link.springer.com/chapter/10.1007/978-3-030-33509-0_60\n', 'A microservices architectural approach providing semi-automated support for the re-engineering of old monolithic applications; for Java and Restful API', 0, 1, 1, 1, 0, 0, NULL, 0, 0, 0, 1, 'Class decomposition and aggregation based on desired level of coupling\n', '1) Decomposition phase: A layered graph model\r\na) Analysis of the source code(abstract syntax tree (AST) and call graph), identify the classes and their  methods (“Entry level\"\")\r\nb) Normalize the number of edges between classes, calculate the communication rate between classes (\"Edge normalization level\")\r\nc) Combine the classes that have a high coupling based on a heuristic variable, repeat until reaching a fair trade of microservices produced and communication rate (\"no merged layer\")\r\nd) Represents the new microservice architecture (\"End merged layer\")\r\n\r\n2) Microservices production and ranking phase\r\na) Produce and order microservices communication stubs\r\nb) Produce a specification document for each microservice\r\nc) Identify deprecated code parts of each microservice using a knowledge base\r\nd) Rank microservices priority for development\r\n\r\n3) Cloud deployment phase\r\na) Semantically represent the identified microservices (OWL-S semantic model for RESTful API)\r\nb) Find equivalent cloud services to replace the microservices of the new architecture (by any semantic representation of applications and cloud services)\r\nc) Produce a standard model for the deployment (OASIS standard “Topology and Orchestration Specification for Cloud Applications???(TOSCA))\r\nd) Automatic creation of TOSCA syntax file for each microservice about its deployment information, produce code in the TOSCA ImplentationArtifact, CSAR packages (executed through the tools that support  TOSCA specification, such as OpenTosca)\"', 1, 0, 0, 0, 0, 'Source code and call graph for code analysis', 0, 1, 'Class', 1, 0, 0, 0, 1, 'Source code and call graph for code analysis', 0, 1, 0, 'Microservice standard model for the deployment\n', 'OASIS standard “Topology and Orchestration Specification for Cloud Applications (TOSCA) + RESTful API: Produce a standard model for the deployment, Automatic creation of TOSCA syntax file for each microservice about its deployment information, produce  code in the TOSCA ImplentationArtifact, CSAR packages (executed through the tools that support  TOSCA specification, such as OpenTosca): for Java, Open API, TOSCA syntax', 'Java + Restful API', 0, 0, 0, 1, NULL, NULL, 1, 1, 0, 1, 1, 1, 'High availability and redundancy, automatic scaling, easier infrastructure management and compliance with the latest security standards ensure a more agile and combined flow of development and operation, also referred to as DevOps\r\n ', 3),
(42, 'From a Monolith to a Microservices Architecture: An Approach Based on Transactional Contexts\n', 2019, 'Lu´ıs Nunes; Nuno Santos; Ant´onio Rito Silva\n', 'https://link.springer.com/chapter/10.1007/978-3-030-29983-5_3\n', 'An Microservices Migration Approach Based on Business Applications Transactional Contexts Identification: for Java + MVC architectural style', 0, 0, 0, 0, 0, 1, NULL, 0, 1, 0, 0, 'Aggregation of domain entities by the transactional contexts', 'Clustering decision: In order to reduce the impact of the migration on the system design we intend to group the domain entities accessed by a controller inside the same microservice, avoiding the introduction of relaxed consistency to the business functionality. Therefore, ideally, a controller would be implemented by a single microservice encapsulating its own database\r\n\r\n1) Data collection\r\nStatic analysis to the monolith source code to collect the information about assessed classes for each controller (Use Java Callgraph tool to generate call graph)\r\n\r\n2) Microservice identification and entity clustering\r\nSimilarity measurement for each pair of domain entities that are accessed by the same controllers, using a hierarchical clustering algorithm (Scipy Python library) to cluster domain entities and  generates a dendrogram\r\n\r\n3) Boundary Cut\r\nSoftware architect can use a user interface to try several cuts in\r\nthe dendrogram to generate different sets of clusters. Allowing the architect to rename, merge and split clusters, as well as move an entity between clusters\r\n\r\n4) Visualization\r\nThe architect visualizes the clusters of entities, accesses pattern of controllers on clusters, impact of domain entities\' data on controllers executing in other clusters\r\n\r\n5) Modeling\r\nThe architect manipulates each one of the views, the tool recalculates the measurement whenever a change is done.', 1, 0, 0, 1, 0, 'Applying the java-callgraph tool for static analysis to the monolith source code\n\nThe architect manipulate clusters and the tool recalculates the measurement', 0, 1, 'Domain Entities and corresponding controllers\r\n', 1, 0, 0, 0, 0, 'Monolith Java source code\n', 0, 1, 0, 'Microservice model\n', '1) Java-callgraph tool: static analysis captures the controllers call graphs (finishing an Eclipse plugin that captures the controllers call graphs using the Eclipse JDT library)\r\n\r\n2) Scipy Python library: hierarchical clustering algorithm which generates a dendrogram\r\n\r\n3) A self-developed visualization tool that allows the rename, merge and split of clusters, and  move of entities between clusters. It also supports different views, cluster, controller and entity, to help on the analysis of the impact of the  decomposition (source code is publicly available in a GitHub repository)', 'Java + MVC architectural style', 0, 0, 1, 0, '1) LdoD archive (a collaborative digital archive that contains the Book of Disquiet, originally written by Portuguese poet, Fernando Pessoa. 152 controllers and 55 domain entities. Java MVC)\r\n\r\n2) Result in 6 services\r\n\r\n3) Do clustering and cutting\r\na) Conduct internal assessment to compare the quality of the clustering resulting from the different cuts by matrices: Number of Singleton Clusters (NSC), Maximum Cluster Size (MCS), Silhouette Score (SS)\r\nb) Conduct external assessment to compare the metrics of an expert decomposition example, the results of our approach and the results by a software architecture analysis tool, Structure1016: pairwise precision, recall and f-score\r\nc) Conduct Visualization Analysis to do a detailed analysis of the decomposition and to identify how the business functionality is split between the different clusters', 'Mean intracluster distance, mean nearest-cluster distance, Silhouette Score (SS), Number of Retrieved Clusters (NRC), Number of Singleton Clusters (NSC), Maximum Cluster Size (MCS), tp (true positives), fp (false positives), fn (false negatives), precision, recall, F-score\n', 0, 0, 0, 0, 0, 0, NULL, 4);
INSERT INTO `contribution` (`id`, `Title`, `Year`, `Authors`, `Link`, `Approach`, `Process_Rewrite`, `Process_Extension`, `Process_StranglerPattern`, `Process_ContinuousEvolution`, `Process_Split`, `Process_Others`, `ProcessDetail`, `Decomposition_DDD`, `Decomposition_FunctionalDecomposition`, `Decomposition_ExistingStructure`, `Decomposition_Others`, `DecompositionDetail`, `TechniqueSet`, `Technique_SCA`, `Technique_MDA`, `Technique_WDA`, `Technique_DMC`, `Technique_Others`, `TechniqueTypeDetail`, `Applicability_GR`, `Applicability_MO`, `AtomicUnit`, `Input_SourceCode`, `Input_UseCase`, `Input_SystemSpecification`, `Input_API`, `Input_Others`, `InputDetail`, `Output_List`, `Output_Archi`, `Output_Others`, `OutputDetail`, `Tool`, `ProgramLanguage`, `Validation_Experiment`, `Validation_Example`, `Validation_CaseStudy`, `Validation_NoValidation`, `ValidationTypeDetail`, `ValidationMetrics`, `Quality_Maintainability`, `Quality_Security`, `Quality_Performance`, `Quality_Reliability`, `Quality_Scalability`, `Quality_Others`, `QualityDetail`, `Score`) VALUES
(43, 'Tool Support for the Migration to Microservice Architecture: An Industrial Case Study\n', 2019, 'Ilaria Pigazzini; Francesca Arcelli Fontana; Andrea Maggioni\n', 'https://link.springer.com/chapter/10.1007/978-3-030-29983-5_17\n', 'Tool Support for the Migration to Microservice Architecture: An Industrial Case Study: for Java\n', 0, 0, 0, 0, 0, 1, 'General Process: Detection of features and domains from existing Java codebases\r ', 1, 0, 0, 0, 'Identify services depending on specific \"domains\" or \"business concerns\" of the project, by considering code as text and by looking for topics that could correspond to services', '1) Input Java Project\r\n\r\n2) Architectural Smell Detection\r\nDetect architectural smells and remove them before the migration process\r\n\r\n3) Dependency Graph Analysis\r\nInspecting the dependency graph with graph queries and by executing graph algorithms to retrieve blocks of the project that are structurally independent and can be reused or transformed in microservices\r\nThree methods:\r\na) Connected Components Detection: Depth First Search (DFS)\r\nalgorithm to find connected components (sets of Java classes or packages), detect and extract totally detached parts of code\r\nb) Vertical Functionality: Depth First Paths (DFP) algorithm isolate and show each functionality to support the extraction of the interested parts of code as microservice candidates\r\nc) Logical Layer View: divide the classes in groups depending on the three tier model by looking at their external dependencies, understand the role of each class\r\n\r\n4) Topic Detection\r\nAutomatically extract specific \"domains\"or \"business concerns\"of the project\r\nThree steps:\r\na) Document collection: selecting comments and source code words from a single Java class (implemented in Java language)\r\nb) Preprocessing: manipulating the text (tokenize, filter) (implemented in Python language)\r\nc) Latent Dirichlet Allocation (LDA)/ Seeded Latent Dirichlet Allocation (SLDA): detect topics represented as word-topic distribution and the document-topic distribution\r\n\r\n5) Candidate Microservices\r\nHints about the semantics to be migrated (which Java classes are associated to a specific domain)', 1, 0, 0, 0, 0, 'Static analysis from existing Java codebases of the system architecture, architectural smell detection and topic detection, a text mining method used here to model software domains', 0, 1, 'Sets of Java classes or packages\n', 1, 0, 0, 0, 0, 'Java codebase\n', 1, 0, 0, 'Information on the Java classes and packages that should belong to the same candidate microservice\n', 'Arcan: a software analysis tool for Java projects migration which relies on graph database technology: it bases all its computations on the dependency graph: for Java Enterprise Edition\n', 'Java', 0, 0, 1, 0, '1) A Java enterprise project: developed to manage the collection of information for the initiation of legal proceedings, composed by 267 classes divided into 27 packages; Daytrader: a project developed by IBM specifically to simulate the manual migration from a Java monolithic architecture to a microservice\n\n2) Daytrader results in 7 services\n\n3) Perform Architectural Smells Detection to detect all the types of architectural smells, use the Vertical Functionality and the Logical Layer view for Dependency Graph Analysis,apply Classic LDA algorithm and Seeded Latent Dirichlet Allocation. Compare results of different algorithm and ask for developer\'s feedback', NULL, 0, 0, 0, 0, 0, 0, NULL, 4),
(44, 'Re-architecting OO Software into Microservices A Quality-Centred Approach', 2018, 'Anfel Selmadji; Abdelhak-Djamel Seriai; Hinde Lilia Bouziane,; Christophe Dony; Rahina Oumarou Mahamane', 'https://link.springer.com/chapter/10.1007/978-3-319-99819-0_5', 'A Quality-Centred Approach for OO software migration, automatically identify microservices from OO source code: for OO source code', 0, 0, 0, 0, 1, 0, NULL, 0, 0, 0, 1, 'Class clustering and grouping', 'For Microservices: Focused on One Function(FOne): Internal Cohesion & Internal Coupling, Data Autonomy (FData): intra-microservice data dependency (FIntra) & Inter-microservice data dependency (FInter), Structural and Behavioral Autonomy (FAutonomy): External Coupling\r\n\r\n1) Use a hierarchical agglomerative clustering algorithm, measure the quality of a microservice as the similarity function used in the algorithm\r\n\r\n2) Based on function of Global Evaluation of a Microservice (Eq. 4), classes that maximize the value of the function are grouped together\r\n\r\nRealized with automatic support, More details can be found in [1] Adjoyan, S., Seriai, A.D., Shatnawi, A.: Service identification based on quality metrics object-oriented legacy system migration towards SOA. In: SEKE (2014)', 1, 0, 0, 0, 0, 'Proposing an automatic approach to identify microservices from OO source code, It is based on a quality function that measures the functional validity of a microservice and its data autonomy', 0, 1, 'Class as atomic unit, granularity as result', 1, 0, 0, 0, 0, 'Object-oriented source code', 1, 0, 0, NULL, 'A hierarchical agglomerative clustering algorithm, rely on OO source code, More details can be found in [1] Adjoyan, S., Seriai, A.D., Shatnawi, A.: Service identification based on quality metrics object-oriented legacy system migration towards SOA. In: SEKE (2014)', 'OO source code', 0, 0, 1, 0, '1) InventoryManagementSystem (104 classes, 19 database tables, 13447 LOC); \nFindSportMates (17 classes, 5 database tables, 785 LOC);\nSpringBlog (42 classes, 5 database tables, 1615 LOC)\n\n2) InventoryManagementSystem (10 microservices for entire quality function evaluation, 9 microservices for quality function without the data autonomy part); \nFindSportMates (2 microservices for entire quality function evaluation, 2 microservices for quality function without the data autonomy part ); \nSpringBlog (4 microservices for entire quality function evaluation, 4 microservices for quality function without the data autonomy part )\n\n3)a) use the tool to identify microservices for each application\nb) generate results of microservice clustering based on entire quality function (FMicro) and on a quality function without the data autonomy part (FSem)\nc) compare the results with those identified manually, use Precision and recall metics to evaluate the good, excellent and bad match microservices', 'Number of excellent, good, bad microservices, Precision, Recall', 0, 0, 0, 0, 0, 1, 'Quality Functions considered for Microservices: Focused on One Function(FOne): InternalCohesion & InternalCoupling, Data Autonomy (FData): intra-microservice data dependency (FIntra) & Inter-microservice data dependency (FInter), Structural and Behavioral Autonomy (FAutonomy): ExternalCoupling', 4),
(45, 'An Experience Report from the Migration of Legacy Software Systems to Microservice Based Architecture', 2019, 'Hugo Henrique S. da SilvaGlauco de F. Carneiro; Miguel P. Monteiro', 'https://link.springer.com/chapter/10.1007/978-3-030-14070-0_26', 'An Experience Report from the Migration of Legacy Software Systems to Microservice Based Architecture: for PHP programming on a Nginx web server\r\n', 0, 0, 1, 1, 0, 0, NULL, 1, 0, 0, 0, NULL, 'Part I Migrating the Legacy System to a Modularized Version\r\n\r\n1) Manually identified the candidate functionalities by navigating among the directories and files to find out boundaries, entities and their main functionalities and responsibilities\r\n\r\n2) Break down the main monolithic module into units using previous identified bounded context (DDD key concepts: aggregate root, value objects and domain services)\r\n\r\n3) Sort them by level of complexity (simpler first)\r\n\r\n4) Create for each of the bounded contexts a folder and within each folder, add three new folders: Domain, Application, Infrastructure. Concentrate and isolate relative source code in each layer\r\n\r\nPart II Migrating from the Modularized version to Microservices\r\n\r\n1) Analysis of the previously developed context map organizational relationship and dependencies, assessment of the feasibility of each context\'s decomposition into microservice candidates\r\n\r\n2) Decompose the monolithic application into smaller service parts based on bounded contexts from DDD\r\n\r\n3) Decoupling the primary feature along its data tables related to each service and create a new database schema (MySQL)\r\n\r\n4) Redirect all front-end applications to the new APIs', 1, 0, 0, 0, 0, 'Manually navigating among the directories and files', 0, 1, 'Business functionalities, granularity based on Single Responsibility Principle', 0, 0, 0, 0, 1, 'Application directories and files', 0, 1, 0, NULL, 'PHP Doctrine Migrations project offer additional functionality on top of the database abstraction layer (DBAL) for versioning your database schema and easily deploying changes to it. It is a very easy to use and a powerful tool.', 'PHP programming on a Nginx web server', 1, 0, 1, 0, '1) eShop system: An online store, browse user authentication, catalog of products special offers, and payments), PHP programming language on a Nginx web server\r\n\r\n2) Result in 6 services\r\n\r\n3) Validation Steps not specified', NULL, 0, 0, 0, 1, 1, 1, 'Use of different frameworks and toolkits and to attain increased scalability and better overall system resilience, increase in\r\nthe autonomy of developing teams within the organization', 3.5),
(47, 'A Reconfigurable Microservice-Based Migration Technique for IoT Systems', 2020, 'Chang-ai Sun; Jing Wang; Jing Guo; Zhen Wang; Li Duan', 'https://link.springer.com/chapter/10.1007/978-3-030-45989-5_12', 'A Reconfigurable Microservice-Based Migration Technique: for Internet of Things (IoT) system', 0, 1, 0, 0, 1, 0, NULL, 1, 0, 0, 0, NULL, '1) Migration to Microservices\r\na) Domain analysis: determines the granularity of microservices in a top-down manner via domain models\r\nb) Static analysis: a bottom-up migration principle, classified into technique analysis, source code analysis, metadata assisted analysis, and data analysis\r\nc) Hierarchy-aware: reserve hierarchy during decomposition of microservices, create an abstraction layer to mask the heterogeneity of multi-source, heterogeneous, and non-uniformly communicated devices, have a partner microservice in the control layer\r\nd) Embedded features-aware: integrates embedded modules for operation controlling or event triggering\r\n\r\n2) Reconfigurable Microservices Compositions\r\na) Variation extensions: decomposed microservices are further extended with variation contexts and registered in the Registration Center\r\nb) Service Composition: a layer of key-value store keeping invocation relationships between microservices\r\nc) Dynamic Configuration: updates key-value pair at run-time through HTTP APIs, bound to a new microservice provider candidate', 1, 1, 1, 1, 0, 'Static analysis: a bottom-up migration principle, classified into technique analysis, source code analysis, metadata assisted analysis, and data analysis\r\n\r\nEnable dynamic configuration to make changes and create reconfigurable IoT System', 0, 1, 'Software and hardware modules, determines the granularity of microservices in a top-down manner via domain models\n', 1, 0, 1, 0, 1, 'Source code, metadata, log data, etc', 0, 0, 1, 'Re-configurable microservice composition', 'CM4MS: a self-developed supporting tool to enable and facilitate the proposed migration technique. Its main features include business process representation, microservice management, and dynamic configuration.', 'Internet of Things (IoT) system', 0, 0, 1, 0, '1) ArduPilot: an open-source unmanned aerial vehicle (UAV) system, IoT systems, 700,000 LOC\n\n2) 10 microservices, 4 layers\n\n3) Conduct Microservice transformation and evaluate Reconfigurability and Performance', 'Performance efficiency (compilation time, startup time, time spent on the variation configuration, response time), Maintainability(Reconfigurability), Reliability (Fault tolerance)', 1, 0, 1, 0, 0, 1, 'Reconfigurability\n', 3),
(48, 'Identifying Microservices Using Functional Decomposition', 2018, 'Shmuel Tyszberowicz; Robert Heinrich; Bo Liu', 'https://link.springer.com/chapter/10.1007/978-3-319-99933-3_4', 'A systematic approach to identify microservices in early design phase', 0, 0, 0, 0, 0, 1, 'Recognize clusters of dense relationships between the required system operations and the state variables that those operations read or write. This provides a partition of the system’s state space into microservices', 0, 1, 0, 0, NULL, '1) Import system specification includes functional requirements (Use case Diagram)\r\n\r\n2) Create a model of the system (a finite set of public operations (methods) that comprise the system’s API and a set of system variables)\r\n\r\n3) Identify the system operations and variables: Use tools (e.g., easyCRC [31], TextAnalysisOnline [39]) that assist us to extract nouns and verbs (suggesting system variables and operations)\r\n\r\n4) Create the operation/relation table\r\n\r\n5) Visualization and System Decomposition: Use NEATO to visualize operation/relation table, build an undirected bipartite graph G. System decomposition as a syntactical partition of the system variables. and guarantee that each microservice has its own system variables and corresponding operations, identify clusters of dense relationships that are weakly connected to other clusters\r\na) low dependency partitions\r\nb) non-functional constraints evaluation\r\nc) explore changes to the system model that reduce the dependencies between areas\r\n\r\n6) APIs and Databases: For each identified microservice we define its API and its data storage\r\n\r\n7) Implement the microservices (using RESTish protocols)\r\n\r\n8) Deploy\r\n\r\n*)The KAMP approach [35] supports software architects assessing the effect of change requests on technical and organizational work areas during software evolution, supports modeling the initial software architecture—the base architecture, and the architecture after a certain change—the target architecture.\r\n', 1, 0, 0, 0, 0, 'Based on use case specification of the software requirements', 1, 1, 'System variables and its operations, granularity is flexibly determined', 0, 1, 1, 0, 0, 'Use case specification of the software requirements', 1, 0, 0, NULL, '1) easyCRC [31], TextAnalysisOnline for verbs and nouns extraction from use case specification\n\n2) NEATO [30]—a spring model based drawing algorithm\n', 'Generic', 0, 0, 1, 0, '1) CoCoME (Common Component Modeling Example) case study, a trading system as can be observed in a supermarket chain handling sales\n\n2) Result in 4 services\n\n3) Perform the decomposition and compare the result with those by three independent software projects, to check whether this approach provides a decomposition of the system into microservices that is similar to a decomposition suggested by humans\n', NULL, 0, 0, 0, 0, 1, 1, 'Traceability is much easier, scalability\n', 4.5),
(50, 'Microservice Decomposition via Static and Dynamic Analysis of the Monolith', 2020, 'Alexander Krause; Christian Zirkelbach; Wilhelm Hasselbring; Stephan Lenga; Dan Kröger\n', 'https://arxiv.org/abs/2003.02603\n', '1) Microservice Decomposition via Static and Dynamic Analysis of the Monolith Application: for Java', 0, 0, 0, 1, 1, 0, 'Source Code Package splitting, grouping and mapping + Continuous Evolution', 1, 0, 0, 0, NULL, '1) Domain analysis\r\na) Familiarization phase: gather domain knowledge and information about application\'s internals and behavior -> Context Diagram (for domain actors and their relationships)\r\nb) Modeling phase: clarify the behavior of application and identified the use cases related to each actor -> Domain Context; denote the relationships at domain context level ->  Context Map (for domain context and their relationships)\r\nc) Partitioning phase: identify target boundaries in the context map -> Bounded Context\r\n\r\n2) Static analysis: validate the boundaries and resolve potential ambiguity at implementation level\r\nEmploy Structure101 (static software structure analysis tool) to analyze the source code packages (SCP), observe SCPs\' dependencies and identify their locations at implementation level, manually mapped the SCPs to the bounded contexts and restructuring some packages to solve domain context conflicts\r\n\r\n3) Dynamic analysis: modularize the monolithic application into a microservice architecture.\r\na) Employ the 3D application-level perspective of ExplorViz to analyze the runtime behavior of the monolithic application, refine the bounded contexts and communication behavior, \r\nb) Decentralize data governance, statically analyzed the data model using DBeaver, map and adjust the tables onto the respective bounded contexts\r\nc) further decompose the resulting software architecture by analyze  runtime behavior to discover additional microservice candidates\r\n', 1, 0, 1, 0, 0, 'Static analysis: validate the boundaries and resolve potential ambiguity at implementation level\n\nDynamic analysis: modularize the monolithic application into a microservice architecture.', 0, 1, 'Source code packages and data tables, granularity adjustable according to level of refinement\r\n', 1, 0, 1, 0, 1, 'Source code + documentation e.g., customer requirements specifications + developer interview', 1, 0, 0, NULL, '1) Structure101: static software structure analysis tool to analyze the source code packages (SCP), levelized structure maps enabled us to observed dependencies among the SCPs and identify their locations at implementation level: for Java\r\n\r\n2) ExplorViz: trace visualization tool enables a live monitoring and visualization of large software landscapes (landscape-level [20] perspective and 3D application-level perspective): for Java\r\n', 'Java', 0, 0, 1, 0, '1) in|FOCUS: a sales and management software solution for lottery providers (monolithic Enterprise Java system)\r\n\r\n2) Not specified number of services\r\n\r\n3) case study to decompose a real java application using the proposed approach\r\n', NULL, 0, 0, 0, 0, 0, 0, NULL, 4),
(53, 'A Design with Mobile Agent Architecture for Refactoring A Monolithic Service into Microservices', 2018, 'Masayuki Higashino; Toshiya Kawato; Takao Kawamura', 'https://pdfs.semanticscholar.org/fd77/d9fe2e0bc594ee3ee60d1f74b1df2c7ce142.pdf\n', 'A Design with Mobile Agent Architecture for Refactoring A Monolithic Service into Microservices: for Mobile Agent web services', 0, 0, 0, 1, 0, 1, 'Splitting up existing Process and Data, Continuous evolution', 1, 0, 0, 0, 'By adopting a mobile agent as a system platform, it is possible to realize a junction capable of achieving both loose couplings in the problem domain and high condensability. It is thought that it can be flexible and easily extracted as a microservice.', '1) Framework for Re-construction\r\nRelaxing restrictions on models based on business logic and models based on hardware and technology being used during microservices decomposition.\r\nA new web framework with constraints and conventions that make it easier to split into microservices. so that configuration becomes possible, and we can migrate a more flexible monolithic service to microservice\r\n\r\n2) Evaluation of Cost with Re-construction\r\nEvaluate the cost of extracting new microservice, estimation of performance degradation by division and measures to prevent irreversible division\r\nBased on the collection and analysis of statistical information of the service, based on the change of the performance cost when dividing microservices, necessary to think about a method to evaluate reversibility.\r\n\r\n3) Dividing and Merging for Process\r\nExtract process as microservice whose function of processing data has the greatest influence on the network distance between the microservice having the data to be processed, the microservice as the processing result output object, and the end user\'s computers.\r\nTrade off between redundancy due to system decentralization and communication delay by remote communication, balance the quality of the domain and the quality of the performance\r\n\r\n4) Dividing and Merging for Data\r\nExtract a data model with a strong association strength between models as a microservice while measuring the frequency of transactions of data generated by multiple data models and the communications traffic of data\r\nClone arbitrary resources by duplicating or dividing it to an arbitrary number, ensuring the connection with the divided resources and an ability of look-up divided resources\r\n\r\n*) In order to localize the scope of influence by service maintenance, it is the goal to dynamically lower the degree of coupling between services and to divide the service while increasing the degree of condensation of the service.\r\n', 0, 0, 1, 0, 0, 'Measuring the frequency of transactions of data generated by multiple data models and the communications traffic of data for process and data dividing and merging', 0, 1, 'Process and data resources\n', 0, 0, 0, 0, 1, 'A conceptual diagram of monolithic mobile agent system', 0, 0, 1, 'A conceptual diagram of distributed dividing and merging of processes as microservices', NULL, 'Mobile Agent web services', 0, 1, 0, 0, NULL, NULL, 1, 0, 0, 0, 0, 0, NULL, 1),
(55, 'From Monolithic Systems to Microservices: A Decomposition Framework based on Process Mining', 2019, 'Davide Taibi; Kari Systä\n', 'https://www.scitepress.org/Papers/2019/77559/77559.pdf', 'A 6-step framework to reduce the subjectivity of the decomposition process', 0, 0, 0, 0, 1, 0, NULL, 0, 0, 0, 1, 'Data-driven approach for identifying microservices candidates based on process mining performed on log files', '0) Use Elastic APM to trace all the activities from the log files\r\n\r\n1) Execution Path Analysis: DISCO was used to graphically represent the business processes by mining the log file, understand Runtime execution paths, Dependencies between the classes, The frequency of usage of each path, Branches and Circular Dependencies\r\n\r\n2) Frequency analysis: output a table representing all the different execution paths with the frequency of their usage\r\n\r\n3) Circular Dependencies Removal: Use a simple algorithm (eg. Inversion of Control (Martin, 2003)) to discover cycles and break these cycles, out put a refined version of the execution path table\r\n\r\n4) Identification of Decomposition Options: identify different decomposition alternatives by visually inspecting the generated graphs (With algorithms or manually, try multiple decomposition options), deal with the duplication of some classes or methods\r\n\r\n5) Metric-based Ranking of the Decomposition Options: three measures to assess the quality of the decomposition options: Coupling - CBM (Coupling Between Microservice), Number of classes per microservices, Number of classes that need to be duplicated\r\n\r\n6) Selection of the Decomposition Solution: decide which solution to adopt by merging or splitting existing processes\r\n', 0, 0, 1, 1, 0, 'Approach the slicing based on runtime behavior\n\nDecide proposed solution to adopt by merging or splitting existing processes', 0, 1, 'Classes or methods, granularity according to quality metrics', 0, 0, 0, 0, 1, 'Process log files\n', 0, 0, 1, 'Microservice decomposition option which best fit the quality metrics\n', '1) Elastic APM for collecting log trace\n\n2) DISCO for graphically representing the business processes by mining the log files\n', 'Generic', 0, 0, 1, 0, '1) A document management system for bookkeeping, for Italian tax accountants, developed by two teams of 4 developers, plus two part-time developers following the moonlight scrum process, developed for more than 12 years and composed of more than 1000 Java classes.\r\n\r\n2) Three manual decomposition options by experts: first solution - 25 microservices (19 classes duplicated), average CBM 0.16; the second solution - 27 microservices (21 classes duplicated), average CBM of 0.16; the third solution - 21 microservices (14 classes duplicated), average CBM of 0.22.\r\n\r\n3) Validation Steps:\r\na) The software architect manually identified the different microservices and a decomposition strategy\r\nb) We applied our 6-steps process to identify different decomposition solutions, and then we compared them to the solution proposed by the software architect\r\nc) The software architect and the project manager provided feedback on the usefulness of our approach\r\n', NULL, 1, 0, 0, 1, 1, 1, 'Maintainability, independent development, scalability and deployment, Reliability (fault-tolerant), reducing the need of interaction between teams\n', 3),
(56, 'A New Decomposition Method for Designing Microservices\n', 2019, 'Omar Al-Debagy; Peter Martinek\n', 'https://pp.bme.hu/eecs/article/view/13925\n', '1) An approach for decomposing monolithic application to a microservices application through analyzing the application programming interface\n\n2) Based on OpenAPI specification\n', 0, 0, 0, 0, 0, 1, 'Splitting up existing API operations\r\n', 0, 1, 0, 0, 'Functional (Operation) Decomposition based on API operation name analysis\n', '1) Input OpenAPI specification\r\n\r\n2) Extract operation names from the specifications\r\n\r\n3) Convert operation names into average word embedding using a given model fastText or Word2Vec to obtain word vector \r\na) Remove stop words and list of specific words from operation name to avoid inaccurate results\r\nb) Apply  fastText or Word2Vec word embedding model to create a sentence level embedding or operation\'s name level embedding\r\nc) Convert operation name into word representation vector by getting the average of word vectors in an operation name\r\n\r\n4) Clustering of semantically similar operation names in order to create candidates of microservice\r\na) Apply the Affinity Propagation Algorithm, find the number of microservices that is going to be created from the API of a monolithic application (\"responsibility\" messages, \"availability\" messages and Self-avaliability)\r\nb) Group similar operation names together (identify the clusters by maximizing the total similarity) and create candidates for microservices\r\nc) Three parameters related to the performance of the algorithm: damping, maximum number of iteration, number of iterations with no change in number of estimated clusters\r\nd) Silhouette coefficient is used to validate data consistency within clusters; Precision, recall and F-measure metrics are used to measure the performance of the decomposition method.', 1, 0, 0, 0, 0, 'Analyze the OpenAPI specification', 1, 1, 'Functional operations, granularities based on Affinity Propagation Algorithm\n', 0, 0, 0, 1, 0, 'OpenAPI specification\n', 1, 0, 0, 'List of miceroservice candidate, granulairty defined by algorithm automatically\n', '1) Affinity Propagation Algorithm: Python with libraries for text analysis and clustering such as Gensim [21], NLTK [22] and Sklearn\n\n2) fastText or Word2Vec word embedding model: Machine learning algorithm which is trained to create a sentence level embedding or operation\'s name level embedding\n', 'Based on OpenAPI specification', 0, 0, 1, 0, 'The test cases were the APIs of four different applications such as Amazon Web Service, Paypal, Kanban Board and Money Transfer app\n\n1) Amazon Web Service, Paypal, Kanban Board and Money Transfer app\n\n2) Money Transfer app: 11 operations, 4 microservices\nKanban Board: 13 operations and 3 microservices\nAmazon Web Service: 318 operations and 47 microservices\nPaypal: 110 operations and 16 microservices\n\n3) Decompose the four applications based on their Api specification, compare the F-Measure of each application by means of fastText or Word2Vec and compare the final decomposition result to the four applications\' existing microservice architecture', 'Silhouette coefficient: a method for validating data consistency within clusters\nPrecision, recall and F-Measure: metrics for evaluating the performance of proposed decomposition method\n\n', 1, 0, 0, 0, 1, 0, NULL, 4),
(57, 'Migration of Software Components to Microservices: Matching and Synthesis\n', 2019, 'Andreas Christoforou; Lambros Odysseos; Andreas S. Andreou\n', 'https://www.semanticscholar.org/paper/Migration-of-Software-Components-to-Microservices%3A-Christoforou-Odysseos/4fcaeb3155899eb62f456423bcbf44e0ada55966\n', 'Migration of Software Components to Microservices: Matching and Synthesis, automatic process that identifies and recommends the full or partial replacement of a software component by a number of available microservices', 0, 0, 0, 1, 0, 0, 'Fully or partially replace their functional parts with by a number of suitable and available microservices', 0, 1, 0, 0, 'Decomposition of a software component into distinct operations and respectively profiles all candidate microservices', '1) Transform Component properties and Microservice Properties into EBNF Profiles\r\nIdentify desired properties for components and microservices, provide their profile, which are categorized into functional, non-functional and other properties. Use the Extended Backus-Naur Form (EBNF) to standardize the description of the properties of the two software artifacts, \r\n\r\n2) Parse the EBNF Profiles into ontology\r\nEmploys a hybrid form combining string and behavioral matching, as a dedicated parser using Another Tool for Language Recognition (ANTLR), to verify the profile\'s context and semantics, recognizes certain parts in the profiles and automatically parse and transform them to ontology tree instances \r\n\r\n3) Isolate Component Ontology and create Component Meta-ontology\r\nDecompose the software component functionalities and the associated non-functional aspects (overall performance indicators, Method-specific indicators), creating a form of meta-ontology (subset of the component’s initial ontology), directly comparable to parts of the microservices ontology\r\n\r\n4) Matching Process\r\nMatch properties of Component Meta-ontology with Microservice Ontology, use an ontology alignment algorithm (eg. Graph Matching for Ontologies (GMO) to calculate attribute similarities and attribute values distances\r\na)Functional constraints: All attributes of the source microservice (component functions) must map one-on-one to the attributes of the target microservice, by traversing all the available target microservices\r\nb)Level of suitability: Different metric function is used for each data type, depending on their data type (binary, numerical and string)\r\n\r\nFinally, discard non-matching microservices from the pool of available candidate microservices.\r\n\r\n5) Composition Assessment (MOGA)\r\nEmploy MOGA (The Non-dominated Sorting Genetic Algorithm II (NSGA-II) and the Strength Pareto Evolutionary Algorithm 2 (SPEA2) as multi-objective optimization algorithm to deliver near-optimal synthesis of candidate microservices taking into account the required dependencies considering the satisfaction of multiple objectives related to non-functional characteristics of the software component\r\n\r\n', 0, 1, 0, 0, 0, 'Use ontologies which are transformed from EBNF Profiles of Component properties and Microservice Properties for microservice analysis', 0, 1, 'Functional parts of interest in the component’s ontology instance which are directly comparable to parts of the microservices ontology, granularity varies according to actual microservice functionalities\r\n', 0, 0, 0, 0, 1, 'Component Profiles and Microservice Profiles\n', 0, 1, 0, NULL, '1) Another Tool for Language Recognition (ANTLR) (http://www.antlr.org/), a parser and translator generator tool that supports language grammars in EBNF syntax.\n', 'Generic', 1, 0, 0, 0, '1) Functional parts of a decomposed CRUD component, which provides the simple functions of create, read, update and delete for a business artifact (e.g. a customer or invoice), no dependencies across the functional parts\r\n\r\nFunctional parts of a component that are part of an inventory system and are dedicated\r\nfor invoice updating. This component consists of five different functions as follows: Update Invoice Items, Update Invoice Headers, Update Corresponded Posting, Update Debtor’s Balance and Print Invoice.\r\n\r\n2) Number of microservice not specified\r\n\r\n3) Apply the framework on these two component, assess the results (score, number of candidate microservices in the pool) of the optimal solution and performance of different algorithms', 'Use number of possible solutions (PS) for the assessment of the microservices synthesis\nUse Hypervolume (HV) (Thiele and Zitzler, 1999) and the Inverted Generational Distance (IGD) (Veldhuizen and Lamont, 2000) quality indicator to assess and compare the performance of the two MOGAs\"\n', 0, 0, 0, 0, 0, 0, NULL, 3),
(58, 'Use Case Driven Microservices Architecture Design\n', 2018, 'Jeremy M. R. MARTIN, Peter J. BOGGIS\n', 'http://wotug.cs.unlv.edu/files/CPA2018/papers/paper-10.pdf', 'Use Case Driven Object Modelling methodology to the decomposition of a system into microservices', 0, 0, 0, 0, 1, 0, 'Split the set of classes into partitions as separate microservices\r\n', 1, 0, 0, 0, 'Use Case Driven Object Modelling methodology - Decomposition of an object-oriented model', '1) Glossary. A glossary of terms to define the business entities and provide a shared vocabulary between the customers and the technical team.\r\n\r\n2) Domain Modelling. The glossary is converted to a domain model diagram which represents the entities (components) and the relationships between them.\r\n\r\n3) Use Case Modelling. The use cases for the system are elucidated.\r\n\r\n4) Robustness Analysis. The domain model is revisited to address missed business entities or \'boundary\' entities.\r\n\r\n5) Sequence Diagrams. The use cases are converted into sequence diagrams which show the tasks to be performed by the various components of the system to be delivered.\r\n\r\n6) Domain decomposition using Aggregated Class Interaction Diagram (ACID) as a  visual tool to graphically identify relationships and possible partition boundaries between components and provide some measure of the interface complexity and balance of the corresponding collection of microservices, clustering based on number of relationships between components, component size, variance of the number of functions of the components', 1, 0, 0, 1, 0, 'The Use Case Driven Object Modelling methodology which provides a systematic, step-by-step approach for converting business analysis outputs into working code\r\nUse ACID for microservice partitioning and selecting a healthy microservices design', 1, 1, 'Functions of  classes, granularity decided manually\n', 0, 0, 1, 0, 0, 'Glossary definition in business language to define the business entities\r\n', 0, 1, 0, NULL, 'Aggregated Class Interaction Diagram (ACID). Provides the development team with a visual tool for selecting a healthy microservices design\n', 'Generic', 0, 1, 0, 0, '1) A system for managing financial models developed by actuaries to simulate a year of business in the London Insurance Market\r\n\r\n2) Result in 8 microservices\r\n\r\n3) Provide an example design of system for illustration\r\n', '1) The number of arcs between components to represent (inversely) the degree of loose-coupling\n\n2) The number of vertices within each component to represent its size\n\n3) The variance of the number of vertices within each component to represent the degree of balance in the system\n', 0, 0, 0, 0, 0, 1, 'Flexibility, more agile and cost-effective\n', 3),
(60, 'Using Microservices for Legacy Software Modernization\n', 2018, 'Holger Knoche; Wilhelm Hasselbring\n', 'https://ieeexplore.ieee.org/document/8354422\n', 'Legacy software incremental modernization to new platform: generic, including Cobol & Java', 0, 0, 0, 0, 1, 0, NULL, 1, 1, 0, 0, NULL, '1) Defining an external service façade:  captures the functionality required by the client systems\r\ncreate a target domain model and used it to define service operations, employ static analysis to identify the \"entry points\" analyze the invoked functionality and to formulate it as provisional service operations, merge similar or redundant operations. \r\n\r\n2) Adapting the service façade: build an implementation by adapting the existing system (Interface preparation)\r\n\r\n3) Migrating clients to the service façade: client applications can start migrating to the new façade by replacing their existing accesses with service invocations (connect to new Interface)\r\n\r\n4) Establishing internal service façades: all programs were inspected manually and assigned to the appropriate components\r\n\r\n5) Replacing the service implementations by microservices: platform migration and database migration', 1, 0, 0, 0, 0, 'Static analysis to identify the \"entry points\" of the application—i.e., programs, methods, or database tables that were accessed from other applications for further migration process', 0, 1, 'Set of facade, business functions, database table, granularity decided by developer manually\n', 1, 0, 0, 0, 1, 'Source code & database', 0, 1, 0, NULL, NULL, 'Generic (Cobol - Java)', 0, 0, 1, 0, '1)customer management application of an insurance company, which provides customer data such as names and postal addresses to the specific insurance applications, developed since 1970s, Cobol+ Java Swing+ Unix servers. 1 million LOC in 1,200 Cobol modules\r\n\r\n2) Result in 23 service operations, mix Java and Cobol code, still have to access the Cobol code to remain compatible with operations that have not yet been migrated. Several technological and organizational barriers still have to be overcome, certain parts of the application cannot be modernized using the presented approach\r\n\r\n3) Validation Steps Not specified\r\n', NULL, 1, 0, 0, 0, 0, 1, 'Deliver new features on time, evolability, reduce complexity, avoid vendor and technology lock-in\n', 3),
(61, 'Migrating Monolithic Mobile Application to Microservice Architecture: An Experiment Report \n', 2017, 'Chen-Yuan Fan; Shang-Pin Ma\n', 'https://ieeexplore.ieee.org/document/8027278\n', 'Migration process based on SDLC (SDLC: software development life cycle), including all the methods and tools required during design, development, and implementation: for Java or node.js\r\n', 0, 0, 0, 1, 1, 0, NULL, 1, 0, 0, 0, NULL, '1) Analysis of the internal system architecture (DDD) [10] to extract candidate microservices from the original system\n\n2) Filtering out of inappropriate candidates based on database schema consistency\n\n3) Extraction and organization of code related to the service candidates\n\n4) Selection of communications protocol, data format, and microservice framework\n\n5) Database adjustments to make appropriate for the new system\n\n6) Java interfaces are transformed into actual service interfaces, such as REST or MQTT\n\n7) Microservice development\n', 1, 1, 0, 0, 0, 'Analysis of the internal system architecture (DDD) [10] to extract candidate microservices from the original system\r\n\r\nExtraction and organization of code related to the service candidates using Java\r\n\r\nThere is no canonical way to split services, which can vary considerably with regard to system requirements, team size, and change requirements\r\n\r\nMany factors determine the extraction of service candidates from existing modules, such as system requirements and the nature of the database', 0, 1, 'Domain modules (class)\n', 1, 0, 1, 0, 1, 'Monolithic system structure & code & requirements\r\n ', 1, 0, 1, 'Candidate list of Microservices and splitted database\n', '1) IDE: IDEA Intellij, Atom\n\n2) Build Tool: Dradle\n\n3) Test Library/Tool: Junit, Mockito, Pact\n\n4) Visualization: Vagrant, Docker, Docker Compose\n\n5) SCM: Git\n\n6) Metrics: Node Exporter, Prometheus, Spring Acutor, cAdvisor, Grafana\n\n7) Log: Filebeat, Logstash, Elasticsearch, Kibana\n\n8) CI/CD: Github -> Jenkins -> Gradle/ Bitbucket/ Dockerhub/ Ansible\n', 'SDLC (software development lifecycle) + Java or Node.js', 0, 0, 1, 0, '1) EasyLearn is a mobile application used to facilitate creation, sharing, reading, discussion, and updating of condensed articles, three subsystems, Android App, Web App, and RESTful services\n\n2) result in 3 microservices\n\n3) Report Migration and report issue and experience\n', 'Maintainability, Reliability (Fault tolerance), Performance efficiency (Resource utilization)', 1, 0, 0, 0, 1, 1, 'Scalability, maintainability, automated testing and deployment, fault tolerance, easy to develop\r\n', 3.5);

--
-- Indexes for dumped tables
--

--
-- Indexes for table `contribution`
--
ALTER TABLE `contribution`
  ADD PRIMARY KEY (`id`),
  ADD UNIQUE KEY `id_UNIQUE` (`id`);

--
-- AUTO_INCREMENT for dumped tables
--

--
-- AUTO_INCREMENT for table `contribution`
--
ALTER TABLE `contribution`
  MODIFY `id` int(11) NOT NULL AUTO_INCREMENT, AUTO_INCREMENT=62;
COMMIT;

/*!40101 SET CHARACTER_SET_CLIENT=@OLD_CHARACTER_SET_CLIENT */;
/*!40101 SET CHARACTER_SET_RESULTS=@OLD_CHARACTER_SET_RESULTS */;
/*!40101 SET COLLATION_CONNECTION=@OLD_COLLATION_CONNECTION */;
